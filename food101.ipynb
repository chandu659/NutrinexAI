{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision matplotlib numpy pandas scikit-learn\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_classes = [\n",
    "    \"apple_pie\", \"baby_back_ribs\", \"baklava\", \"beef_carpaccio\",\n",
    "    \"beignets\", \"bibimbap\", \"caesar_salad\", \"caprese_salad\",\n",
    "    \"carrot_cake\", \"ceviche\", \"cheesecake\", \"chicken_curry\",\n",
    "    \"chicken_quesadilla\", \"churros\", \"clam_chowder\", \"club_sandwich\",\n",
    "    \"crab_cakes\", \"donuts\", \"eggs_benedict\", \"falafel\"\n",
    "]\n",
    "\n",
    "\n",
    "base_path = \"food-101/images\"\n",
    "filtered_paths = []\n",
    "labels = []\n",
    "\n",
    "for cls in selected_classes:\n",
    "    class_path = os.path.join(base_path, cls)\n",
    "    images = os.listdir(class_path)\n",
    "    for img in images:\n",
    "        filtered_paths.append(os.path.join(class_path, img))\n",
    "        labels.append(cls)\n",
    "\n",
    "\n",
    "data = pd.DataFrame({\"image_path\": filtered_paths, \"label\": labels})\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, stratify=data[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx][\"image_path\"]\n",
    "        label = self.dataframe.iloc[idx][\"label\"]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, selected_classes.index(label)\n",
    "\n",
    "\n",
    "train_dataset = FoodDataset(train_df, transform=train_transforms)\n",
    "test_dataset = FoodDataset(test_df, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.get_device_name(0))  \n",
    "\n",
    "\n",
    "class FoodClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FoodClassifier, self).__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "model = FoodClassifier(num_classes=len(selected_classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, test_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_losses[-1]:.4f}, Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def estimate_portion_size(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "    if width * height < 150000:\n",
    "        return \"Small\"\n",
    "    elif width * height < 300000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Large\"\n",
    "\n",
    "\n",
    "def predict(image_path, model, transform, classes):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    outputs = model(image)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    label = classes[predicted.item()]\n",
    "    return label\n",
    "\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "    image_path = file_name  \n",
    "\n",
    "\n",
    "predicted_label = predict(image_path, model, test_transforms, selected_classes)\n",
    "portion_size = estimate_portion_size(image_path)\n",
    "\n",
    "\n",
    "print(f\"Predicted: {predicted_label}, Portion Size: {portion_size}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
